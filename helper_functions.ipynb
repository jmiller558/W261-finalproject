{"cells":[{"cell_type":"code","source":["from pyspark.ml.evaluation import Evaluator\n\n# Import libraries for running the helper functions\nfrom pyspark.sql.functions import min, max, col, expr, when, udf, create_map, lit, explode, array\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number, unix_timestamp, isnan, count, concat, desc\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import StructType, StructField, FloatType, IntegerType, DoubleType, StringType\nfrom operator import itemgetter\nfrom pyspark import SparkFiles\nimport os\nfrom timezonefinder import TimezoneFinder\nimport pytz\nfrom datetime import datetime, date, timedelta\nimport numpy as np\nimport sys\nfrom graphframes import *\n\n# Additional imports for the tree pipeline\nfrom sparkdl.xgboost import XgboostClassifier\nfrom pyspark.ml.evaluation import Evaluator\nfrom pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\nfrom pyspark.ml.feature import VectorAssembler, VectorIndexer, StringIndexer, OneHotEncoder\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\n# Additional imports for displaying and plotting results\nimport time\nimport pandas as pd\nimport seaborn as sns\nfrom pyspark.mllib.evaluation import MulticlassMetrics\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8fd973e-8e62-43f6-825c-32eaba4d3c65"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def add_airport_icao_coords(us_airport_locs, airlines):\n    \"\"\"Add the coordinates, timezones, and ICAO codes for the origin and destination airports\"\"\"\n    \n    # Alias \n    df1_a = airlines.alias(\"df1_a\")\n    df2_a = us_airport_locs.alias(\"df2_a\")\n\n    # Add the latitude and longitude coordinates to the 3 mo dataset for the origin airports\n    airlines_2 = df1_a.join(df2_a, df1_a.ORIGIN == df2_a.IATA, 'inner')\\\n                      .withColumnRenamed('ICAO', 'origin_ICAO')\\\n                      .withColumnRenamed('IATA', 'origin_IATA')\\\n                      .withColumnRenamed('latitude','origin_latitude')\\\n                      .withColumnRenamed('longitude','origin_longitude')\\\n                      .withColumnRenamed('tz_database_timezone', 'origin_local_timezone')\n  \n    # Realias\n    df3_a = airlines_2.alias(\"df3_a\")\n  \n    # Add the latitude and longitude coordinates to the 3 mo dataset for the origin airports\n    airlines_3 = df3_a.join(df2_a, df3_a.DEST == df2_a.IATA, 'inner')\\\n                      .withColumnRenamed('ICAO', 'destination_ICAO')\\\n                      .withColumnRenamed('IATA', 'destination_IATA')\\\n                      .withColumnRenamed('latitude', 'destination_latitude')\\\n                      .withColumnRenamed('longitude', 'destination_longitude')\\\n                      .withColumnRenamed('tz_database_timezone', 'destination_local_timezone')\n  \n    # Drop the IATA column added from the us_airport_locs dataset\n    airlines_3 = airlines_3.drop('origin_IATA', 'destination_IATA')\n  \n    return airlines_3\n\ndef nearest_origin_station(airlines, stations):\n    \"\"\"Add the closest weather station to each origin airport to the flights table\"\"\"\n    \n    # Alias the tables in preparation for the join\n    df1_a = airlines.alias(\"df1_a\")\n    df2_a = stations.alias(\"df2_a\")\n   \n    # Join the closest stations features to the airlines dataframe based upon the origin airport\n    # Use an inner join to drop flight observations that do not have a closest origin weather station (since we are interested in the effect of weather upon flight outcomes)\n    airlines_2 = df1_a.join(df2_a, df1_a.origin_ICAO == df2_a.neighbor_call, 'inner')\n    airlines_2 = airlines_2.withColumnRenamed('station_id', 'origin_weather_station')\n    airlines_2 = airlines_2.drop('neighbor_call', 'origin_ICAO', 'distance_to_neighbor')\n    return airlines_2\n\ndef nearest_destination_station(airlines, stations):\n    \"\"\"Add the closest weather station to each destination airport to the flights table\"\"\"\n  \n    # Rename the columns to avoid Spark error\n    stations = stations.withColumnRenamed('neighbor_call', 'destination_call')\n  \n    # Alias \n    df3_a = airlines.alias(\"df3_a\")\n    df2_a = stations.alias(\"df2_a\")\n  \n    # Join the closest stations features to the airlines dataframe based upon the destination airport\n    # # Use an inner join to drop flight observations that do not have a closest destination weather station (since we are interested in the effect of weather upon flight outcomes)\n    airlines_2 = df3_a.join(df2_a, df3_a.destination_ICAO == df2_a.destination_call, 'inner')\n    airlines_2 = airlines_2.withColumnRenamed('station_id', 'destination_weather_station')\n    airlines_2 = airlines_2.drop('destination_neighbor_call', 'destination_ICAO', 'distance_to_neighbor')\n    return airlines_2\n\ndef add_cutoff_window(airlines):\n    \"\"\"Add columns representing the UTC timestamps of the cutoff window, for flight departure\"\"\"\n    \n    # Create a field representing the flight's scheduled departure in UTC time and add columns for the UTC timestamps 2 and 3 hours before scheduled departure\n    airlines = airlines.withColumn('scheduled_departure', F.to_timestamp(F.concat(airlines.FL_DATE, F.lpad(airlines.CRS_DEP_TIME, 4, '0')), format='yyyy-MM-ddHHmm'))\\\n                       .withColumn('two_hrs_prior', col('scheduled_departure') + expr('INTERVAL -2 HOURS'))\\\n                       .withColumn('three_hrs_prior', col('scheduled_departure') + expr('INTERVAL -3 HOURS'))\n  \n    airlines = airlines.withColumn('utc_scheduled_departure', F.to_utc_timestamp(col('scheduled_departure'), col('origin_local_timezone')))\\\n                       .withColumn('utc_2hrs_before', F.to_utc_timestamp(col('two_hrs_prior'), col('origin_local_timezone')))\\\n                       .withColumn('utc_3hrs_before', F.to_utc_timestamp(col('three_hrs_prior'), col('origin_local_timezone')))\\\n                       .drop('scheduled_departure', 'two_hrs_prior', 'three_hrs_prior')\n \n    return airlines"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a35d517c-e75e-464e-9ec0-4b6f8dfaf18d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def parse_concatenated_fields(weather):\n    \"\"\"Select only the weather records of interest and create separate fields for the columns in the weather dataset containing concatenated fields\"\"\"\n    \n    # Extract only U.S. weather observations and select only the columns of interest\n    weather = weather.filter(weather.NAME.contains('US')).select(weather.columns[0:16])\n    \n    # Split the concatenated fields\n    weather_col = F.split(weather['WND'], ',')\n    cig_col = F.split(weather['CIG'], ',')\n    vis_col = F.split(weather['VIS'], ',')\n    temp_col = F.split(weather['TMP'], ',')\n    dew_col = F.split(weather['DEW'], ',')\n    slp_col = F.split(weather['SLP'], ',')\n  \n    # Add the parsed, split fields as new columns and name\n    # Drop the original/unparsed columns\n    weather = weather.withColumn('wind_dir', weather_col.getItem(0).cast('int'))\\\n                     .withColumn('wind_dir_qual', weather_col.getItem(1).cast('int'))\\\n                     .withColumn('wind_obs_type', weather_col.getItem(2))\\\n                     .withColumn('wind_obs_speed', weather_col.getItem(3).cast('int'))\\\n                     .withColumn('wind_obs_speed_qual', weather_col.getItem(4).cast('int'))\\\n                     .withColumn('ceil_height', cig_col.getItem(0).cast('int'))\\\n                     .withColumn('ceil_qual', cig_col.getItem(1).cast('int'))\\\n                     .withColumn('ceil_ok', cig_col.getItem(3))\\\n                     .withColumn('vis_dist', vis_col.getItem(0).cast('int'))\\\n                     .withColumn('vis_dist_qual', vis_col.getItem(1).cast('int'))\\\n                     .withColumn('vis_dist_var', vis_col.getItem(2))\\\n                     .withColumn('vis_dist_var_qual', vis_col.getItem(3).cast('int'))\\\n                     .withColumn('air_temp', temp_col.getItem(0).cast('int'))\\\n                     .withColumn('air_temp_qual', temp_col.getItem(1))\\\n                     .withColumn('dew_pt_temp', dew_col.getItem(0).cast('int'))\\\n                     .withColumn('dew_pt_qual', dew_col.getItem(1))\\\n                     .withColumn('sea_level_p', slp_col.getItem(0).cast('int'))\\\n                     .withColumn('sea_level_p_qual', slp_col.getItem(1).cast('int'))\\\n                     .drop('WND', 'CIG', 'VIS', 'TMP', 'DEW', 'SLP')\n    \n    # Drop rows with missing values in the fields of interest\n    weather = weather.filter(\"wind_obs_type <> '9' and wind_obs_speed <> 9999 and ceil_height<>99999 and vis_dist<>999999 and air_temp<>'+9999' and dew_pt_temp<>'+9999'\")\\\n                     .select(\"STATION\",\"DATE\",\"LATITUDE\",\"LONGITUDE\",\"ELEVATION\",\"NAME\",\"QUALITY_CONTROL\",\"wind_obs_type\",\"wind_obs_speed\",\"ceil_height\",\"vis_dist\",\"air_temp\",\"dew_pt_temp\")\n\n    return weather"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9521edf3-1631-4adf-8050-cba101928309"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def airport_importance(df, num_iter):\n    '''\n    use PageRank to determine airport importance\n    inputs:\n    * df: spark df, original\n    * num_iter: integer for pagerank input, example uses num_iter = 5\n    output:\n    * df2: spark df, after adding pagerank_origin and pagerank_dest columns, ready for imbalance processing\n    '''\n    #Create vertices and edges with selected helper columns - ORIGIN, DEST, utc_scheduled_departure\n    vertices = df.select(explode(array(\"ORIGIN\", \"DEST\")).alias(\"id\"))\\\n                        .dropDuplicates()\n    edges = df.withColumn(\"id\", F.concat(col(\"ORIGIN\"), lit(\"_\"), col(\"DEST\"), lit(\"_\"), col(\"utc_scheduled_departure\")))\\\n                    .withColumn(\"outcomes\", df[\"OUTCOME\"].cast(IntegerType()))\\\n                    .selectExpr(\"id\",\"ORIGIN as src\",\"DEST as dst\",\"DISTANCE\",\"outcomes\")\n    #Create graph\n    df_graph = GraphFrame(vertices, edges)\n    \n    #Measure the importance of airports by determining which vertices(airports) have the most edges (trips) with other vertices\n    ranks = df_graph.pageRank(resetProbability=0.15, maxIter=num_iter)\n    df_ranks = ranks.vertices\n    \n    #Append airport importance to feature selected table\n    #df1 - append pagerank for origin first; df2 - append pagerank for destination\n    df1 = df.join(df_ranks, df.ORIGIN == df_ranks.id).select(df[\"*\"],df_ranks[\"pagerank\"].alias(\"pagerank_origin\"))\n    df2 = df1.join(df_ranks,df1.DEST == df_ranks.id).select(df1[\"*\"], df_ranks[\"pagerank\"].alias(\"pagerank_dest\"))\n    \n    #Drop ORIGIN, DEST, utc_scheduled_departure columns\n    df2 = df2.drop(\"ORIGIN\",\"DEST\",\"utc_scheduled_departure\")\n    \n    return df2"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"506ece18-7b51-49e6-b820-74786271f2b6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def upsample(df):\n    \"\"\"A function that upsamples a dataframe's minority class to create a balanced dataset\"\"\"\n    \n    # Get the dataframes corresponding to the majority and minority classes\n    major_obs = df.filter(col('OUTCOME') == 0)\n    minor_obs = df.filter(col('OUTCOME') == 1)\n    \n    # Get the fraction of all observations corresponding to the minor class\n    minor_count = minor_obs.count()\n    major_count = major_obs.count()\n    minor_perc = minor_count/float(minor_count + major_count)\n    \n    # Oversample the minority class to balance the number\n    oversampled_df = minor_obs.sample(withReplacement = True, fraction = minor_perc, seed = 42).union(major_obs)\n    \n    return oversampled_df\n\ndef downsample(df):\n    \"\"\"A function that downsamples a dataframe's majority class to create a balanced dataset\"\"\"\n    \n    # Get the dataframes corresponding to the majority and minority classes\n    major_obs = df.filter(col('OUTCOME') == 0)\n    minor_obs = df.filter(col('OUTCOME') == 1)\n    \n    # Find the ratio of major class to minor class observations\n    ratio = int(major_obs.count()/minor_obs.count())\n    # Downsample the majority class\n    reduced_majority = major_obs.sample(False, 1/ratio, seed=42)\n    # Create a new dataset, with fewer majority class observations\n    reduced_df = reduced_majority.unionAll(minor_obs)\n    \n    return reduced_df\n\ndef smote_rdd(df):\n    \"\"\"Implement SMOTE using sci-kit learn nearest neighbor, RDD operations, and downsampling as necessary\"\"\"\n    \n    # Filter the dataframe for minority and majority class observations\n    major_obs = df.filter(col('label') == 0)\n    minor_obs = df.filter(col('label') == 1)\n    \n    # Get the count of major class observations\n    major_counts = major_obs.count()\n    minor_counts = minor_obs.count()\n\n    # There cannot be more nearest neighbors than there are examples of the minority class\n    try_k = int(major_counts/minor_counts)\n    if try_k > minor_counts:\n        k = minor_counts\n    else:\n        k = try_k\n    \n    # Select the features field from the dataframe of minority class observations and convert to an RDD\n    featureMin = minor_obs.select('features').rdd\n    # Extract the features and create a numpy array from them\n    feature_val = featureMin.map(lambda x: x[0]).collect()\n    feature = np.asarray(feature_val)\n    \n    # Create a nearest neighbors classifier and fit to the minority class features\n    nearest_nbrs = neighbors.NearestNeighbors(n_neighbors=k).fit(feature)\n    # Get the k-nearest neighbors for each row\n    row_nbrs =  nearest_nbrs.kneighbors(feature)\n    row_nbrs = row_nbrs[1]\n    \n    # Create a list to hold the new rows\n    newRows = []\n    \n    for idx in range(len(feature_val)):\n        for i in row_nbrs[idx]:\n            newRec = feature_val[idx] + ((feature_val[idx] - feature_val[i])*random.random())\n            newRows.insert(0,(newRec))\n    # Create a new RDD from the new rows\n    newData_rdd = sc.parallelize(newRows)\n    # Create a dataframe from the RDD of the new rows\n    newData_rdd_new = newData_rdd.map(lambda x: Row(features = x, label = 1))\n    new_data = newData_rdd_new.toDF()\n    # Join the synthetic features back to the minor observations\n    new_data_minor = minor_obs.select('features', 'label').unionAll(new_data)\n    \n    # Find the count of records in the new dataframe\n    minor_counts = new_data_minor.count()\n    \n    # Find the ratio of major class to synthetic + original minor class observations\n    try:\n        ratio = int(major_counts/minor_counts)\n        \n        # Select only the necessary columns from the major dataframe\n        major_df = major_obs.select('features', 'label')\n    \n        # If the ratio is approximately 1, join the new dataframe of synthetic + original minority class observations to the major class observations\n        if ratio <= 1:      \n            smoted_df = major_df.unionAll(new_data_minor)\n        else:\n            # Downsample the majority class\n            reduced_majority = major_df.sample(False, 1/ratio)\n            # Create a new dataset, with fewer majority class observations\n            smoted_df = reduced_majority.unionAll(new_data_minor)\n    \n        return smoted_df\n    except ZeroDivisionError:\n        return (f'There were no minority class observations in the sample')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"443a0c4b-6e87-44f2-b070-5d0634a077b9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def calculate_metrics(df):\n    \"\"\"A custom function to calculate recall, accuracy, and f1-score\"\"\"\n    \n    # Convert String to Integer Type\n    df = df.withColumn('label',col('label').cast(IntegerType()))\\\n           .withColumn('prediction', col('prediction').cast(IntegerType()))\n    \n    # Group by actual and predicted labels and get the counts of each pair\n    cols = df.select('label', 'prediction').groupby('label', 'prediction').count()\n    \n    # Get the counts of each classification type\n    tn = cols.filter((col('label') == 0) & (col('prediction') == 0)).collect()[0][2]\n    tp = cols.filter((col('label') == 1) & (col('prediction') == 1)).collect()[0][2]\n    fn = cols.filter((col('label') == 1) & (col('prediction') == 0)).collect()[0][2]\n    fp = cols.filter((col('label') == 0) & (col('prediction') == 1)).collect()[0][2]\n    \n    # Calculate accuracy, recall, and f1-score\n    if tn + tp + fn + fp == 0:\n        accuracy, recall, f1, precision = 0, 0, 0, 0\n    else:\n        accuracy = float(tp + tn)/float(tp + fp + tn + fn)\n    \n    if (tp + fn) > 0:\n        recall = (tp)/float(tp+fn)\n    elif (tp + fn) == 0:\n        recall = 0\n    if (tp + fp + fn) > 0:\n        f1_score = (tp)/float(tp + (0.5 * (fp + fn)))\n    elif (tp + fp + fn) == 0:\n        f1_score == 0\n    if (tp + fp) > 0: \n        precision = float(tp)/float(tp + fp)\n    elif (tp + fp) == 0:\n        precision = 0\n    \n    return f1_score, recall, accuracy, precision"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3fbdc7d8-1aba-4d3d-8b70-b93978e4069e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["class RecallEvaluator(Evaluator):\n    \"\"\"A custom class to use recall as the evaluation function\"\"\"\n    \n    def __init__(self, predictionCol=\"prediction\", labelCol=\"label\"):\n        self.predictionCol = predictionCol\n        self.labelCol = labelCol\n\n    def _evaluate(self, dataset):\n        \"\"\"\n        Finds the recall score on the dataset\n        \"\"\"\n        # Count the true positives, true negatives, false positives, and false negatives\n        tp = dataset.filter((F.col(self.labelCol).cast(IntegerType()) == 1) & (F.col(self.predictionCol).cast(IntegerType()) == 1)).count()\n        fp = dataset.filter((F.col(self.labelCol).cast(IntegerType()) == 0) & (F.col(self.predictionCol).cast(IntegerType()) == 1)).count()\n        fn = dataset.filter((F.col(self.labelCol).cast(IntegerType()) == 1) & (F.col(self.predictionCol).cast(IntegerType()) == 0)).count()\n        # Get the recall score\n        if (tp + fn) > 0:\n            recall = float(tp)/float(tp+fn)\n        else: recall = 0\n        return recall\n\n    def isLargerBetter(self):\n        return True"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c8777a4f-62a3-4181-a046-f38d5314cb88"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def check_outcome(dep_del15, cancelled, diverted):\n    \"\"\"Bin cancelled, diverted, and delayed flights with a delay of 15+ minutes\"\"\"\n    x = 0\n    if dep_del15 == 1 or cancelled == 1 or diverted == 1:\n        x = 1\n    return x"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"101df43a-4d7c-4c29-82be-0e2e6daa3b29"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"helper_functions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1858507102381816}},"nbformat":4,"nbformat_minor":0}
